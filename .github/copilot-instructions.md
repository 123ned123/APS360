# AI Coding Agent Guide

- **Purpose & layout**: Course-style PyTorch notebooks for APS360. Key notebooks live under [Tuts/tut1](Tuts/tut1), [Tuts/tut2](Tuts/tut2), [Tuts/tut3](Tuts/tut3), [Tuts/tut4](Tuts/tut4); experimental transformer toy lives in [Building_Mini_GPT.ipynb](Building_Mini_GPT.ipynb). [lab1](lab1) is currently empty.
- **Environment**: Python + PyTorch/torchvision. Use the local venv if present or run `pip install -r requirements.txt` (it is heavy; for notebooks you usually just need torch, torchvision, matplotlib, numpy). CUDA is optional; code falls back to CPU.
- **Data locations**: MNIST is downloaded on demand into `data/` relative to each tutorial. Transfer learning expects Caltech-256 extracted under [Tuts/tut3/data/caltech256](Tuts/tut3/data/caltech256) with class subfolders already present. Large data and env folders are ignored in [.gitignore](.gitignore); do not check in datasets or checkpoints.
- **Tutorial 2 (MLP classifier)**: [Tuts/tut2/Tut2_Multi_Class_ANN.ipynb](Tuts/tut2/Tut2_Multi_Class_ANN.ipynb) slices MNIST into 4,096 train / 1,024 val, flattens images to 28*28, and trains a 3-layer MLP (ReLU, SGD+momentum). Accuracy helpers run on torch DataLoaders; training plots losses/accuracies and saves weights to `mnist_classification_model.pth`. Keep new classifiers consistent with these shapes and batching.
- **Tutorial 3 (Transfer learning sandbox)**: [Tuts/tut3/Tut3_Transfer_Learning.ipynb](Tuts/tut3/Tut3_Transfer_Learning.ipynb) defines `PracANN` on 256x256 RGB inputs, uses `ImageFolder` loaders with random split (80/20) and GPU if available. It seeds with 20, uses SGD with momentum, and plots training/validation curves. Maintain device handling (`device = cuda if available else cpu`) and pin_memory gating by device.
- **Tutorial 4 (Autoencoder)**: [Tuts/tut4/Tut_4_Autoencoder.ipynb](Tuts/tut4/Tut_4_Autoencoder.ipynb) builds two autoencoders (layered and simple) on MNIST, limits data to 4,096 items, flattens to 28*28, and trains with MSELoss + Adam (weight_decay=1e-5). Includes unused RNN/Transformer skeletons; keep them untouched unless intentionally extended.
- **Mini GPT toy**: [Building_Mini_GPT.ipynb](Building_Mini_GPT.ipynb) implements a tiny causal Transformer (`MiniGPT`) with positional encoding, trained on a short integer sequence and greedy decoding. If extending, preserve the causal mask in `_generate_square_subsequent_mask` and batch-first transformer usage.
- **General notebook patterns**: `torch.manual_seed` set for reproducibility; images flattened via `view(-1, 28*28)` for MNIST; DataLoaders constructed from list slices for quick epochs; training loops follow `loss.backward()`, `optimizer.step()`, `optimizer.zero_grad()` and plot via matplotlib.
- **Assets & outputs**: Model weights live beside notebooks (e.g., [Tuts/tut2/mnist_classification_model.pth](Tuts/tut2/mnist_classification_model.pth)). Test images for MNIST inference sit in [Tuts/tut2/test_num_4.png](Tuts/tut2/test_num_4.png) and [Tuts/tut2/test_unicorn.png](Tuts/tut2/test_unicorn.png).
- **Working style**: Prefer updating existing notebooks over adding scripts; keep code blocks self-contained with visible imports. When adding new notebooks, mirror the existing seed/device/setup cells so plots and results are reproducible.
- **Running notebooks**: Start Jupyter/Lab from repo root (`jupyter notebook`), open the desired notebook, and run cells sequentially. For faster iteration, lower `num_epochs` and dataset slices; keep batch sizes aligned with existing loaders (64 for MNIST, 32 for Caltech256).
- **Data hygiene**: Do not move or rename dataset folders without updating loaders. Avoid committing outputs, data, or new checkpoints; `.gitignore` already excludes `data/`, `.venv/`, and caches.
- **When extending**: Reuse helper functions (`get_accuracy`, `train`) rather than duplicating logic. Preserve tensor shapes expected by each model (flattened MNIST vs 3x256x256 RGB). Keep plotting minimal and gated to notebooks to avoid CLI dependencies.
